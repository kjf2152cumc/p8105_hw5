---
title: "Homework 5 - P8105"
author: "Kaleb J. Frierson"
date: "2024-11-12"
output: 
  github_document: 
    toc: TRUE
--- 

# Introduction

This is the Iteration Unit homework. This unit includes the components Writing Functions, Iteration & List columns, and Simulation.

## Library Calling

Here are all libraries used throughout this RMD: 
```{r libraries, message=FALSE}

library(tidyverse)
library(readxl)
library(rvest)
library(broom)

```

# Problem 1

In this problem I will use a `function` to obtain and plot summary statistics from a `sample`. 

## Random Function

Before building the function I practice using `sample` to build a sample with 100 numbers 1 to 365 to represent days of an entire calendar year: 

```{r sample}
n=100
bday_sample = sample(1:365, n, replace = TRUE)

bday_sample

```
Here I write a `function` that, for n = 100, randomly draws “birthdays” for each person; checks whether there are duplicate birthdays in the group; and returns `TRUE` or `FALSE` based on the result:

```{r function}

bday_repeats = 
  function(n = 100) {
  
    bday_sample = 
      sample(1:365, n, replace = TRUE)
    
    has_duplicates = 
    any(duplicated(bday_sample))
  
  return(has_duplicates)
}

bday_repeats()     
bday_repeats(1)    
bday_repeats(32)  

```

## Using Function & Plotting Results

Now I run this `function` 10000 times for each group size between 2 and 50. For each group size, I compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs. Then I use ggplot to generate a `geom_line` that shows the probability as a function of group size.

```{r}

group_n = 2:50
simulations = 10000

bday_sim_results = tibble(group_size = group_n) |> 
  rowwise() |> 
  mutate(
    probability = mean(replicate(simulations, bday_repeats(group_size)))
  ) |> 
  ungroup()

ggplot(bday_sim_results, aes(x = group_size, y = probability)) +
  geom_line(color = "blue") +
  labs(
    title = "Probability of At Least Two People Sharing a Birthday",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()

```


**Comments on Graph:** As group size increases from n = 2 to n = 50, the probability that someone in the group shares a birthday with someone else increases. 

# Problem 2

In this problem, I will conduct a simulation to explore power in a one-sample t-test.

## Set Elements & Generate Datasets

Below I generate 5000 datasets from the model: 

𝑥∼𝑁𝑜𝑟𝑚𝑎𝑙[𝜇,𝜎]

In the `function` I  set the following design elements:

Fix 𝑛=30
Fix 𝜎=5
Set 𝜇=0

For each dataset, I save 𝜇̂ and the p-value arising from a test of 𝐻:𝜇=0 using 𝛼=0.05

To obtain the estimate and p-value, I use `broom::tidy` to clean the output of `t.test`: 

```{r}

t_simulation = 
  function(mu = 0, n = 30, sigma = 5, simulations) {
  if (!is.numeric(n) || n <= 0) stop("Sample size 'n' must be a positive number.")
  if (!is.numeric(sigma) || sigma <= 0) stop("Standard deviation 'sigma' must be a positive number.")
  if (!is.numeric(simulations) || simulations <= 0) stop("Number of simulations must be a positive number.")
  
  map_dfr(1:simulations, ~ { 
    x = rnorm(n, mean = mu, sd = sigma)
    test_result = t.test(x, mu = 0)
    tidy_res = tidy(test_result)
    
    tibble( 
      mu_true = mu, 
      mu_hat = tidy_res$estimate, 
      p_value = tidy_res$p.value
    )
  })
}

test_result = 
  t_simulation(mu = 0, n = 30, sigma = 5, simulations = 5000)

head(test_result)

```

## Repetitive Tasks & Plotting

Here I repeat the above for 𝜇={1,2,3,4,5,6}: 

```{r using function}

mu_values = 1:6

repeated = 
  map_dfr(mu_values, ~ t_simulation(.x, n=30, sigma=5, simulations=5000)) |> 
  unnest(cols = p_value)

head(repeated)
```

Here I generate a plot using `gglpot` showing power on the y axis and the true value of 𝜇 on the x axis: 
```{r power plot}

power_results = 
  repeated |> 
  group_by(mu_true) |> 
  summarize(power = mean(p_value < 0.05), .groups = "drop") 

power_plot = 
power_results |> 
ggplot(aes(x = mu_true, y = power)) +
  geom_point(color = "goldenrod") +
  geom_line(color = "royalblue") +
  labs(
    title = "Power of the Test vs. True Value of µ",
    x = "True Value of µ",
    y = "Power"
  ) +
  theme_minimal()

power_plot

```


**Describe the association between effect size and power**: Power increases as the true mean, a proxy for effect size, increases. 

Here I make a plot showing the average estimate of 𝜇̂ on the y axis and the true value of 𝜇on the x axis. I then overlay the average estimate of 𝜇̂ only in samples for which the null was rejected on the y axis and the true value of 𝜇 on the x axis:

```{r}

average_results = 
  repeated |> 
  group_by(mu_true) |> 
  summarize(
    avg_mu_hat = mean(mu_hat),            
    avg_mu_hat_rejected = mean(mu_hat[p_value < 0.05])
  )

plot = 
  average_results |> 
  ggplot(aes(x = mu_true)) +
  geom_line(
    aes(y = avg_mu_hat, color = "Overall Avg µ̂"), 
            linetype = "dashed", linewidth = 1) +
  geom_line(
    aes(y = avg_mu_hat_rejected, color ="Avg µ̂ (Null   Rejected)"), size = 1) +
  labs(
    title = "Average Estimate of µ̂ vs True Value of µ",
    x = "True Value of µ",
    y = "Average Estimate of µ̂", 
    color = "Legend"
  ) +
  theme_minimal() 

plot

```


**Is the sample average of 𝜇̂ across tests for which the null is rejected approximately equal to the true value of 𝜇? Why or why not?**

Not when true value of mu is lower than about 3. This makes sense because we showed earlier that power (correctly rejecting a null hypothesis) increases with effect size. 

# Problem 3

## Describe Data

Here I use `mutate` to create a city_state variable. Then I `summarize` within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”):

```{r}

murder = 
  read.csv("local data/homicide-data.csv", na = c("NA", "", ".")) |> 
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) |> janitor::clean_names()
  
homicide_summary = 
  murder |>   
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved),
    .groups = "drop"
  )

knitr::kable(homicide_summary)

```


**Describe the raw data:** There are ` r nrow(murder)` rows and `r ncol(murder)` columns in the `murder` dataset. Chicago, IL has the most homicides in the dataset: 5535 with a shocking 4073 of them unsolved. 

## Function: prop.test

For the city of Baltimore, MD, I use the `prop.test` function to estimate the proportion of homicides that are unsolved; I save the output of     `prop.test` as an R object called `baltimore_test`, and then apply `broom::tidy` to that object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}

baltimore_data = 
  homicide_summary |> 
  filter(city_state == "Baltimore, MD")

baltimore_test = prop.test(
  x = baltimore_data$unsolved_homicides,
  n = baltimore_data$total_homicides
)

baltimore_results = 
  tidy(baltimore_test) |> 
  select(estimate, conf.low, conf.high)

baltimore_results
```

## Run Function on Cities

Here I run `prop.test` for each of the cities in the dataset and extract both the proportion of unsolved homicides and the confidence interval for each. I do this within a “tidy” pipeline, making use of `purrr::map2`.  Probably above the needs of this class, I set criteria such that `binom.test` is run on those with less than 5 values as to not violate assumptions of `prop.test`. Estimated proportions and CIs are provided for each city:

```{r}
city_results = 
  homicide_summary |> 
  mutate(
    prop_test = map2(
      unsolved_homicides, 
      total_homicides, 
      ~ if (.x < 5 | (.y - .x) < 5) {
          tidy(binom.test(x = .x, n = .y))
        } else {
          tidy(prop.test(x = .x, n = .y))
        }
    )
  ) |> 
  unnest(prop_test) |> 
  select(city_state, estimate, conf.low, conf.high)

head(city_results)

```

## Plot

Here I create a plot showing estimates and CIs for each city. `geom_errorbar` allowed me to add error bars based on the upper and lower limits. Cities were organized according to the proportion of unsolved homicides using `arrange`. For best viewership, see the knitted document. 

```{r}
city_results = 
  city_results |> 
  arrange(desc(estimate)) |> 
  mutate(city_state = factor(city_state, levels = city_state))

city_results |> 
ggplot(aes(x = city_state, y = estimate)) +
  geom_point(color = "blue") +
  geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Proportion of Unsolved Homicides"
  ) +
  theme_minimal()
```

